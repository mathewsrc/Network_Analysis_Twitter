{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Network_Analysis_Twitter/blob/master/02_semanticnetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "3gNOVxdOLfr1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEr-dIfv90fP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df6788c-6669-43e0-ee7c-4d8a4d4bf85c"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "  import pyvis\n",
        "  from pyvis.network import Network\n",
        "except:\n",
        "  !pip install pyvis\n",
        "  import pyvis\n",
        "  from pyvis import Network\n",
        "from time import sleep\n",
        "import nltk\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "import re\n",
        "import shutil\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "import itertools\n",
        "punctuation = string.punctuation\n",
        "stopwordsset = set(stopwords.words(\"english\"))\n",
        "stopwordsset.add('rt')\n",
        "stopwordsset.add(\"'s\")\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2iBARi-s-aX"
      },
      "source": [
        "WORKING_DIR = \"/content/drive/MyDrive/MSDS_marketing_text_analytics/master_files/3_network_analysis\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget http://128.138.93.164/nikelululemonadidas_tweets.jsonl.gz -P /content/drive/MyDrive/MSDS_marketing_text_analytics/master_files/3_network_analysis"
      ],
      "metadata": {
        "id": "VoZUyrxq0-c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!gzip -d /content/drive/MyDrive/MSDS_marketing_text_analytics/master_files/3_network_analysis/nikelululemonadidas_tweets.jsonl.gz"
      ],
      "metadata": {
        "id": "XePW56jB8QXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Prep"
      ],
      "metadata": {
        "id": "OgBGBHTBLl-j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1QDCkXKIzkw"
      },
      "source": [
        "#Removing urls\n",
        "def removeURL(text):\n",
        "  result = re.sub(r\"http\\S+\", \"\", text)\n",
        "  return result\n",
        "\n",
        "#Extracting contextual words from a sentence\n",
        "# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n",
        "def tokenize(text):\n",
        "  #lower case\n",
        "  text = text.lower()\n",
        "  #split into individual words\n",
        "  words = word_tokenize(text)\n",
        "  return words\n",
        "\n",
        "#stem - peaches : peach : reduce the number of repeated words\n",
        "def stem(tokenizedtext):\n",
        "  rootwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = ps.stem(aword)\n",
        "    rootwords.append(aword)\n",
        "  return rootwords\n",
        "\n",
        "#removes useless words such as a, an, the\n",
        "def stopWords(tokenizedtext):\n",
        "  goodwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in stopwordsset:\n",
        "      goodwords.append(aword)\n",
        "  return goodwords\n",
        "\n",
        "# feature reduction. taking words and getting their roots and graphing only the root words\n",
        "def lemmatizer(tokenizedtext):\n",
        "  lemmawords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = wn.lemmatize(aword)\n",
        "    lemmawords.append(aword)\n",
        "  return lemmawords\n",
        "\n",
        "#inputs a list of tokens and returns a list of unpunctuated tokens/words\n",
        "def removePunctuation(tokenizedtext):\n",
        "  nopunctwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in punctuation:\n",
        "      nopunctwords.append(aword)\n",
        "  cleanedwords = []\n",
        "  for aword in nopunctwords:\n",
        "    aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
        "    cleanedwords.append(aword)\n",
        "    \n",
        "  return cleanedwords\n",
        "\n",
        "def removesinglewords(tokenizedtext):\n",
        "  goodwords = []\n",
        "  for a_feature in tokenizedtext:\n",
        "    if len(a_feature) > 1:\n",
        "      goodwords.append(a_feature)\n",
        "  return goodwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open('%s/nikelululemonadidas_tweets.jsonl' % WORKING_DIR, 'r')"
      ],
      "metadata": {
        "id": "vQBfHnic15F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a lot of metadata in a tweet! If you want to take a look at all that you can get from a tweet:\n",
        "https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet"
      ],
      "metadata": {
        "id": "iEbCsuo_8zom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniquewords = {}\n",
        "\n",
        "for i, atweet in enumerate(json_file):\n",
        "    if i % 10000 == 0:\n",
        "      print(i)\n",
        "    tweetjson = json.loads(atweet)\n",
        "    text = tweetjson['full_text']\n",
        "    #natural language pre processing : clean the tweet \n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "    \n",
        "    # print(tokenizedtext)\n",
        "    # print(nostopwordstext)\n",
        "    # print(lemmatizedtext)\n",
        "    # print(nopuncttext)\n",
        "    # sleep(10)\n",
        "\n",
        "    for aword in nopuncttext:\n",
        "        if aword in uniquewords:\n",
        "            uniquewords[aword] += 1\n",
        "        if aword not in uniquewords:\n",
        "            uniquewords[aword] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSrz-9ofjsKs",
        "outputId": "a37a2861-9e49-4242-bc43-1e0ca4fbe4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(uniquewords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIOWn78gn182",
        "outputId": "f3e89ffe-1ac3-4841-e70f-e36da6fdb1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98470"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  list of unique words: key - word; value = number of times the word appears.\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "    if uniquewords[aword] > 300: # play with this number to get around 1000 words\n",
        "        wordcount += 1\n",
        "        wordstoinclude.add(aword)"
      ],
      "metadata": {
        "id": "mN2SV6G1n5yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordcount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmRL97vJn7xr",
        "outputId": "f3869263-a8b1-4ed7-9c4a-953e5551a9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open('%s/nikelululemonadidas_tweets.jsonl' % WORKING_DIR, 'r')"
      ],
      "metadata": {
        "id": "OIWez2NNpSGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Graph = nx.Graph()"
      ],
      "metadata": {
        "id": "-9P4974Qo4D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniquewords = {}\n",
        "\n",
        "for i, atweet in enumerate(json_file):\n",
        "    if i % 10000 == 0:\n",
        "      print(i)\n",
        "    tweetjson = json.loads(atweet)\n",
        "    text = tweetjson['full_text']\n",
        "    #natural language pre processing : clean the tweet \n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "    \n",
        "    # print(tokenizedtext)\n",
        "    # print(nostopwordstext)\n",
        "    # print(lemmatizedtext)\n",
        "    # print(nopuncttext)\n",
        "    # sleep(10)\n",
        "    \n",
        "    goodwords = []\n",
        "    for aword in nopuncttext:\n",
        "        if aword in wordstoinclude:\n",
        "            goodwords.append(aword.replace(',', ''))\n",
        "\n",
        "    allcombos = itertools.combinations(goodwords, 2) # set this to the number of words you want in each combination\n",
        "    for acombo in allcombos:\n",
        "        row = []\n",
        "        for anode in acombo:\n",
        "            row.append(anode)\n",
        "        Graph.add_edge(row[0], row[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPuwXCTaoZ09",
        "outputId": "f1b7482c-a962-47e7-9600-779e4ba22b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.info(Graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HCORIS0yb_S5",
        "outputId": "4615ae48-f9b5-4170-98af-0829fba6a7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Graph with 923 nodes and 240694 edges'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(300, 300))\n",
        "#ugh these settings are such a pain, but here's the way to look them up: \n",
        "#https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
        "nx.draw_networkx(Graph, ax=ax, font_color=\"#FFFFFF\", font_size=20, node_size=30000, width=4, arrowsize=100)\n",
        "plt.savefig(\"%s/semantic_network.png\" % WORKING_DIR, format=\"PNG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbhvxJFle9TU",
        "outputId": "97569b61-d967-40d8-896f-80d63faf6980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 8294 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 8294 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 8297 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 8297 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128293 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128293 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128081 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128081 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128548 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128548 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 129300 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 129300 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128064 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128064 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 129321 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 129321 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128248 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128248 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128079 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128079 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128095 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128095 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128591 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128591 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 129315 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 129315 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 127873 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 127873 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 127998 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 129392 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 127998 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 129392 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 128640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 128640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.show()"
      ],
      "metadata": {
        "id": "iSKbFijZrep-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt = Network('1000', '600', directed=True, notebook=True, layout='force_atlas_2based')\n",
        "# populates the nodes and edges data structures\n",
        "nt.from_nx(Graph)\n",
        "nt.show_buttons(filter_=['physics'])\n",
        "#nt.show('nx.html')\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML('nx.html'))"
      ],
      "metadata": {
        "id": "Nhdi-fDLQnDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}